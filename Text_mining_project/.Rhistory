length(Abstract)
size(tokens)
dim(tokens)
# Transform to a matrix and inspect.
tokens.matrix <- as.matrix(tokens.dfm)
# Create our first bag-of-words model dataframe.
# tokens <- tokens[1:(length(tokens)/2)]
tokens.dfm <- dfm(tokens)
# Transform to a matrix and inspect.
tokens.matrix <- as.matrix(tokens.dfm)
library(XML)
library(easyPubMed)
library(ggplot2)
# Option 2: 52349 documents via importation du fichier xml.
#----------
papers <- xmlParse(file = "/home/francois/Documents/Projet_Text_mining/pubmed18n0924.xml")
## Information Extraction from dataset ("papers")
#------------------------------------------------
xmltop = xmlRoot(papers) # top node of "papers" xml structure
Article_Num <- xmlSize(xmltop) # number of nodes (Articles) "in papers"
ID <- vector()
Abstract <- vector()
Title <- vector()
Date <- vector()
Author_lastname <- vector()
Author_forename <- vector()
Author <- vector()
for (i in 1:Article_Num) {
ID[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["PMID"]])
Abstract[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["Abstract"]])
Title[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["ArticleTitle"]])
Date[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["ArticleDate"]])
Author_lastname[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["AuthorList"]][["Author"]][["LastName"]])
Author_forename[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["AuthorList"]][["Author"]][["ForeName"]])
Author[i] <- paste(Author_lastname[i],Author_forename[i])
}
rm(papers)
papers
# create dataframe
df <- data.frame(ID, Abstract, Title, Date, Author)
rm(ID, Abstract, Title, Date, Author)
rm(ID, Abstract, Title, Date, Author, Author_forename, Author_lastname)
# Remove Na's and too long or too short Abstracts.
df <- df[complete.cases(df[ , 2]),]
df <- df[nchar(as.character(df[ , 2]))<3000 & nchar(as.character(df[ , 2]))>100,]
library(quanteda)
Abstract <- as.character(df$Abstract)
NbrDoc<-100
# Tokenize
tokens <- tokens(Abstract, what = "word",
remove_numbers = TRUE, remove_punct = TRUE,
remove_symbols = TRUE, remove_hyphens = FALSE)
# minimize capital letters
tokens <- tokens_tolower(tokens)
# Tokenize
tokens <- tokens(Abstract, what = "word",
remove_numbers = TRUE, remove_punct = TRUE,
remove_symbols = TRUE, remove_hyphens = FALSE)
# minimize capital letters
tokens <- tokens_tolower(tokens)
# Stopwords
stop<-stopwords()
new_stopwords<-append(stop,c("fig.","eq.","e.g"))
tokens <- tokens_select(tokens, new_stopwords, selection = "remove")
tokens <- tokens_select(tokens,min_nchar = 2, selection ="keep")
# Create our first bag-of-words model dataframe.
# tokens <- tokens[1:(length(tokens)/2)]
tokens.dfm <- dfm(tokens)
# Transform to a matrix and inspect.
tokens.matrix <- as.matrix(tokens.dfm)
# Create our first bag-of-words model dataframe.
tokens <- tokens[1:(length(tokens)/2)]
length(tokens)
library(XML)
library(easyPubMed)
library(ggplot2)
############### PART 1: Information extraction ###############
## Dataset importation
#---------------------
# Option 1: 698 documents via une requete
#----------
# Querry_String <- "AIDS"
# Ids <- get_pubmed_ids(Querry_String)
# papers <- fetch_pubmed_data(Ids)
# Option 2: 52349 documents via importation du fichier xml.
#----------
papers <- xmlParse(file = "/home/francois/Documents/Projet_Text_mining/pubmed18n0924.xml")
## Information Extraction from dataset ("papers")
#------------------------------------------------
xmltop = xmlRoot(papers) # top node of "papers" xml structure
Article_Num <- xmlSize(xmltop) # number of nodes (Articles) "in papers"
# xmlSApply(xmltop[[1]], xmlName) # shows names of child nodes
ID <- vector()
Abstract <- vector()
Title <- vector()
Date <- vector()
Author_lastname <- vector()
Author_forename <- vector()
Author <- vector()
for (i in 1:Article_Num) {
ID[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["PMID"]])
Abstract[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["Abstract"]])
Title[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["ArticleTitle"]])
Date[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["ArticleDate"]])
Author_lastname[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["AuthorList"]][["Author"]][["LastName"]])
Author_forename[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["AuthorList"]][["Author"]][["ForeName"]])
Author[i] <- paste(Author_lastname[i],Author_forename[i])
}
rm(papers)
# create dataframe
df <- data.frame(ID, Abstract, Title, Date, Author)
rm(ID, Abstract, Title, Date, Author, Author_forename, Author_lastname)
# Remove Na's and too long or too short Abstracts.
df <- df[complete.cases(df[ , 2]),]
df <- df[nchar(as.character(df[ , 2]))<3000 & nchar(as.character(df[ , 2]))>100,]
# visualize abstract lengths
a<- vector()
for (i in 1:length(df$Abstract)) {a[i]<-nchar(as.character(df$Abstract[i]))}
plot(a)
############### PART 2: text mining  ###############
## Bag of words approach:
#------------------------
library(quanteda)
Abstract <- as.character(df$Abstract)
NbrDoc<-100
# Tokenize
tokens <- tokens(Abstract, what = "word",
remove_numbers = TRUE, remove_punct = TRUE,
remove_symbols = TRUE, remove_hyphens = FALSE)
# for bigrams.
# test.tokens <- tokens_ngrams(test.tokens, n = 1:2)
# minimize capital letters
tokens <- tokens_tolower(tokens)
# Stopwords
stop<-stopwords()
new_stopwords<-append(stop,c("fig.","eq.","e.g"))
tokens <- tokens_select(tokens, new_stopwords, selection = "remove")
tokens <- tokens_select(tokens,min_nchar = 2, selection ="keep")
# Steming
# tokens <- tokens_wordstem(tokens, language = "english")
# print(tokens)
library(XML)
library(easyPubMed)
library(ggplot2)
############### PART 1: Information extraction ###############
## Dataset importation
#---------------------
# Option 1: 698 documents via une requete
#----------
# Querry_String <- "AIDS"
# Ids <- get_pubmed_ids(Querry_String)
# papers <- fetch_pubmed_data(Ids)
# Option 2: 52349 documents via importation du fichier xml.
#----------
papers <- xmlParse(file = "/home/francois/Documents/Projet_Text_mining/pubmed18n0924.xml")
## Information Extraction from dataset ("papers")
#------------------------------------------------
xmltop = xmlRoot(papers) # top node of "papers" xml structure
Article_Num <- xmlSize(xmltop) # number of nodes (Articles) "in papers"
# xmlSApply(xmltop[[1]], xmlName) # shows names of child nodes
ID <- vector()
Abstract <- vector()
Title <- vector()
Date <- vector()
Author_lastname <- vector()
Author_forename <- vector()
Author <- vector()
for (i in 1:Article_Num) {
ID[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["PMID"]])
Abstract[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["Abstract"]])
Title[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["ArticleTitle"]])
Date[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["ArticleDate"]])
Author_lastname[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["AuthorList"]][["Author"]][["LastName"]])
Author_forename[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["AuthorList"]][["Author"]][["ForeName"]])
Author[i] <- paste(Author_lastname[i],Author_forename[i])
}
rm(papers)
# create dataframe
df <- data.frame(ID, Abstract, Title, Date, Author)
rm(ID, Abstract, Title, Date, Author, Author_forename, Author_lastname)
# Remove Na's and too long or too short Abstracts.
df <- df[complete.cases(df[ , 2]),]
df <- df[nchar(as.character(df[ , 2]))<3000 & nchar(as.character(df[ , 2]))>100,]
# visualize abstract lengths
a<- vector()
for (i in 1:length(df$Abstract)) {a[i]<-nchar(as.character(df$Abstract[i]))}
plot(a)
############### PART 2: text mining  ###############
## Bag of words approach:
#------------------------
library(quanteda)
Abstract <- as.character(df$Abstract)
NbrDoc<-100
# Tokenize
tokens <- tokens(Abstract, what = "word",
remove_numbers = TRUE, remove_punct = TRUE,
remove_symbols = TRUE, remove_hyphens = FALSE)
# for bigrams.
# test.tokens <- tokens_ngrams(test.tokens, n = 1:2)
# minimize capital letters
tokens <- tokens_tolower(tokens)
# Stopwords
stop<-stopwords()
new_stopwords<-append(stop,c("fig.","eq.","e.g"))
tokens <- tokens_select(tokens, new_stopwords, selection = "remove")
tokens <- tokens_select(tokens,min_nchar = 2, selection ="keep")
# Steming
# tokens <- tokens_wordstem(tokens, language = "english")
# print(tokens)
rm(a)
source('~/Documents/Projet_Text_mining/Text-Mining-with-R/LSA/BrouillonPUBMED.R', echo=TRUE)
runApp("~/Desktop/Shinyapp")
library(shiny)
runApp("~/Desktop/Shinyapp")
View(tokens)
tokens.dfm <- dfm(tokens)
View(tokens.dfm)
as.matrix(tokens.dfm,sparse = TRUE)
t <- as.matrix(tokens.dfm,sparse = TRUE)
object.size(t)
tbig <- as.matrix(tokens.dfm)
object.size(tbig)
m2 <- Matrix(0, nrow = 1000, ncol = 1000, sparse = TRUE)
m2 <- matrix(0, nrow = 1000, ncol = 1000, sparse = TRUE)
library('Matrix')
m1 <- matrix(0, nrow = 1000, ncol = 1000)
m2 <- Matrix(0, nrow = 1000, ncol = 1000, sparse = TRUE)
object.size(m1)
object.size(m2)
# Transform to a matrix and inspect.
tokens.matrix <- as.matrix(tokens.dfm)
View(tokens)
# Transform to a matrix and inspect.
tokens.matrix <- as.matrix(tokens.dfm)
tokens.dfm <- dfm(tokens)
# Transform to a matrix and inspect.
tokens.matrix <- as.matrix(tokens.dfm)
knitr::opts_chunk$set(echo = TRUE)
plot(pressure)
shiny::runApp('Documents/Projet_Text_mining/Text-Mining-with-R/Text_mining_project/GUI')
setwd("~/Text-Mining-with-R/Text_mining_project")
loadPackage <- dget("Source/loadPackage.R")
loadPackage("shiny","topicmodels","dplyr","data.table","easyPubMed","XML","quanteda","tm","foreach","doParallel")
setwd("~/Text-Mining-with-R/Text_mining_project")
loadPackage <- dget("Source/loadPackage.R")
loadPackage("shiny","topicmodels","dplyr","data.table","easyPubMed","XML","quanteda","tm","foreach","doParallel")
df <- readRDS("Data/Dataframe") # row data
loadPackage <- dget("Source/loadPackage.R")
setwd("~/Text-Mining-with-R/Text_mining_project")
setwd("/home/francois/Documents/Projet_Text_mining/Text-Mining-with-R/Text_mining_project")
loadPackage <- dget("Source/loadPackage.R")
loadPackage("shiny","topicmodels","dplyr","data.table","easyPubMed","XML","quanteda","tm","foreach","doParallel")
df <- readRDS("Data/Dataframe") # row data
irlba <- readRDS("Data/irlba") # SVD matrix (for LSA)
irlba <- readRDS("/home/francois/Documents/Projet_Text_mining/irlba") # SVD matrix (for LSA)
ap_documents <- readRDS("Data/LDAdoc") # for LDA
ap_top_terms <- readRDS("Data/LDAtop_terms")
## CSS
mycss <- "
#error{color: #F5A52A;
font-size: 13px;
}
#loadmessage {
position: relative;
border-radius: 25px;
margin-left: auto;
margin-right: auto;
top: 0px;
left: 0px;
width: 50%;
padding: 5px 0px 5px 0px;
text-align: center;
font-weight: bold;
font-size: 100%;
color: #000000;
background-color: #F5F5F5;
z-index: 105;
}
#pubmed_img{
position: relative;
top: -54px;
left: 115px;
}
"
Strings <- data.frame("noPosQuery" = "Your positive querry isn't significant in any of our topics, try an other research"
, "noNegQuery" = "Sorry, we will not take into account the negative request"
, "insignificantNegQuery" = "Your negative querry isn't significant in any of our topics, try an other research or continue")
runApp('GUI')
setwd("/home/francois/Documents/Projet_Text_mining/Text-Mining-with-R/Text_mining_project")
setwd("/home/francois/Documents/Projet_Text_mining/Text-Mining-with-R/Text_mining_project")
## Extraction or load
loadPackage <- dget("Source/loadPackage.R")
loadPackage("quanteda","tm","foreach","doParallel","XML","easyPubMed","dplyr")
extract_data <- FALSE       # TRUE if new load needed
queryPUBMED <- ''           # keep empty if you want full database
abstractSize <- c(100,3000) # min and max caracter in abstracts analysed
new_Tokens <- FALSE         # if you want to recompute tokenization
stemming <- FALSE           # to stem tokens
new_LSA <- FALSE             # TRUE if you want to recalculate LSA
nv <- 100                   # number of dimensions for LSA
flag <- TRUE                # working version   ---------------------> TODO: find the bug in LSA.R
new_LDA <- FALSE             # TRUE if you want to recalculate LDA
k <- 20L                    # hyper parameter for LDA
LSAquery <- TRUE
LDAquery <- FALSE            # to activate queries
interactiveQueries <- TRUE # to activate interactive queries
## ---- QUERIES --------- ##
# give a positive & negative query as a vector of strings ('query','query',...)
posQuery_String <- ('cancer')
negQuery_String <- ('') # '' for no negative query
if (interactive() & interactiveQueries){
if (LDAquery) cat("LDA only allows one positive keyword and one negative keyword \n")
posQuery_String <- readline("Give a positive query:")
negQuery_String <- readline("Give a negative query:")
}
loadPackage <- dget("Source/loadPackage.R") # use loadPackage instead of library
############# Data extraction ####################
#------------------------------------------------#
if (extract_data == FALSE & file.exists("Data/Dataframe")){
df <- readRDS("Data/Dataframe")
}else{
cat("new extraction \n")
Extract_Data <- dget("Source/Extract_Data.R")
df <- Extract_Data(queryPUBMED,abstractSize)
# export dataframe
saveRDS(df, file = "Data/Dataframe", ascii = FALSE, version = NULL,
compress = TRUE, refhook = NULL)
}
############## Tokenization ######################
#------------------------------------------------#
if (new_Tokens | file.exists("Data/tokensDF") == FALSE){
flag0 <- TRUE
tokenization <- dget("Source/tokenization.R")
tokensDF <- tokenization(df,stemming,flag0)
saveRDS(tokensDF, file = "Data/tokensDF", ascii = FALSE, version = NULL,
compress = TRUE, refhook = NULL)
} else tokensDF <- readRDS("Data/tokensDF")
################### LSA ##########################
#------------------------------------------------#
if (new_Tokens | new_LSA | file.exists("Data/irlba") == FALSE){
## Data processing (preprocessing & SVD)
if (flag) {
loadPackage("irlba")
## analyzing Tokens:
#-------------------
# Our function for calculating relative term frequency (TF)
term.frequency <- function(row) {
row / rowSums(row)
}
# Our function for calculating inverse document frequency (IDF)
inverse.doc.freq <- function(col) {
corpus.size <- length(col[,1])
doc.count <- colSums(col > 0)
log10(corpus.size / doc.count)
}
# Our function for calculating TF-IDF.
tf.idf <- function(tf, idf) {
tf * idf
}
cat("tf-idf \n")
# First step, normalize all documents via TF.
tokens.tf <- term.frequency(tokensDF)
# Second step, calculate the IDF vector that we will use - both
tokens.idf <- inverse.doc.freq(tokensDF)
# Lastly, calculate TF-IDF for our training corpus.
tokens.tfidf <- tf.idf(tokens.tf,tokensDF)
## Perform SVD. Specifically, reduce dimensionality down to 'nv' columns
#-----------------------------------------------------------------------
# for our latent semantic analysis (LSA).
cat("SVD \n")
irlba <- irlba(tokens.tfidf, nv = nv, maxit = 1000)
# line names
rownames(irlba$v) <- colnames(tokensDF)
rownames(irlba$u) <- row.names(tokensDF)
saveRDS(irlba, file = "Data/irlba", ascii = FALSE, version = NULL,
compress = TRUE, refhook = NULL)
} else{
LSA <- dget("Source/LSA.R")
irlba <- LSA(tokensDF,nv)
}
}else {
irlba <- readRDS("Data/irlba")
}
#################### LDA #########################
#------------------------------------------------#
if (new_LDA | file.exists("Data/LDAtop_terms") == FALSE){
runLDA <- dget("Source/LDA.R")
runLDA(k,tokensDF)
}else {
LDAtop_terms <- readRDS("Data/LDAtop_terms")
LDAdoc <- readRDS("Data/LDAdoc")
}
############### Query system #####################
#------------------------------------------------#
Abstract <- as.character(df$Abstract)
if (LSAquery){
query_system <- dget("Source/LSA_query_system.R")
Result <- query_system(irlba,posQuery_String,negQuery_String,Abstract,stemming)
cat("Positive queries:",posQuery_String,"\n","Negative queries:",negQuery_String,"\n")
for (i in (1:10)) {
cat("Result",i,"\n","Abstract",Result[i],"\n",Abstract[Result[i]],"\n")
}
}
if (LDAquery){
query_system <- dget("Source/LDA_query_system.R")
query_system(posQuery_String,negQuery_String,LDAtop_terms,LDAdoc,Abstract)
}
runApp('GUI')
runApp('GUI')
Strings <- data.frame(  "noPosQuery" = "Your positive querry isn't significant in any of our topics, try an other research"
, "noNegQuery" = "Sorry, we will not take into account the negative request"
, "insignificantNegQuery" = "Your negative querry isn't significant in any of our topics, try an other research or continue")
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
runApp('GUI')
df$Date[1:input$max_Results]
df$Date[1:3]
df$Date[100:102]
df$Date[100:101]
df$Date[100]
a<-df$Date[100]
a
as.Date(a, "%m-%d-%Y")
as.Date(a)
as.Date(a, "%Y")
a<-as.Date(df$Date[100:110], "%Y")
as.Date(df$Date[100:110], "%Y")
as.Date(df$Date[103], "%Y")
df$Date[103]
df$Date[102]
df$Date[101]
a<-as.Date(df$Date[101], "%Y")
as.Date(df$Date[101], "%Y")
df$Date[101]
as.Date(as.character(df$Date[101]), format="%Y%m%d")
df$Date[101]
as.Date(as.character(df$Date[101]), format="%Y%m%d")
as.Date(as.character(df$Date[101:115]), format="%Y%m%d")
runApp('GUI')
runApp('GUI')
df <- readRDS("Data/Dataframe") # row data
as.Date(as.character(df$Date[101]), format="%Y%m%d")
as.Date(as.character(df$Date[1:100]), format="%Y%m%d")
df$Date[1:100]
plot(df$Date[1:100])
plot(df$Date[1:1000])
plot(df$Date[1:10000])
plot(df$Date[1:100000])
plot(df$Date[1:1000000])
plot(df$Date[1000:1000000])
plot(df$Date[1:1000000])
as.Date(as.character(df$Date[1:1000]), format="%Y%m%d")
as.Date(as.character(df$Date[1:100000]), format="%Y%m%d")
as.Date(as.character(df$Date[1]), format="%Y%m%d")
as.Date(as.character(df$Date[100]), format="%Y%m%d")
as.character.Date(as.character(df$Date[100]), format="%Y%m%d")
as.Date(as.character(df$Date[100]), format="%Y%m%d")
df$ID[1:10]
as.Date(as.character(df$Date[1:10]), format="%Y%m%d")
as.Date(as.character(df$Date[1:20]), format="%Y%m%d")
as.Date(as.character(df$Date[1:50]), format="%Y%m%d")
df$Date[1:50]
as.Date(as.character(df$Date[1]), format="%Y%m%d")
as.Date(as.character(df$Date[32]), format="%Y%m%d")
as.Date(df$Date[32], format="%Y%m%d")
as.Date(df$Date[1:32], format="%Y%m%d")
runApp('GUI')
runApp('GUI')
runApp('GUI')

---
title: "Main_RMarkdown"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

Through this RMarkdown document we will explain step by step the functioning of our project

The first thing to do before launching is to set the working directory to the "Text_mining_project" directory which contains all the needed scripts. 
```{r setup, include=FALSE}
# Set working directory.
setwd("~/Text-Mining-with-R/Text_mining_project")
```

Next we load all the necessary packages. For this we have built a simple function that will check if the needed packages are installed on the user's computer and if not it will install them. The following packages are required: quanteda tm foreach doParallel XML easyPubMed dplyr.
```{r}
# Package loading and/or installing.
loadPackage <- dget("Source/loadPackage.R")
loadPackage("quanteda","tm","foreach","doParallel","XML","easyPubMed","dplyr")
```

The following section is a sort of control panel that allows for an easier acces to the most critical parameters of the process. This is a way to update or to modify the database for the experienced user.
This section is linked to the following parts by a "flag" system.
```{r}
# Data extraction
extract_data <- FALSE       # TRUE if new load needed
queryPUBMED <- ''           # keep empty if you want full database
abstractSize <- c(100,3000) # min and max caracter in abstracts analysed
# Pre-processing
new_Tokens <- FALSE         # if you want to recompute tokenization
stemming <- FALSE           # to stem tokens
# LSA
new_LSA <- FALSE            # TRUE if you want to recalculate LSA
nv <- 100                   # number of dimensions for LSA
flag <- TRUE                # working version   --------------------- TODO: find the bug in LSA.R
# LDA
new_LDA <- FALSE            # TRUE if you want to recalculate LDA
k <- 20L                    # hyper parameter for LDA
# Query
LSAquery <- TRUE
LDAquery <- FALSE            # to activate queries
interactiveQueries <- FALSE   # to activate interactive queries
```

This section allows the experienced user to perform queries within the terminal in order to run quick tests on the database without having to use the GUI.
```{r}
# Queries
# give a positive & negative query as a vector of strings ('query','query',...)
posQuery_String <- ('cancer')
negQuery_String <- ('') # '' for no negative query

if (interactive() & interactiveQueries){
  if (LDAquery) cat("LDA only allows one positive keyword and one negative keyword \n")
  posQuery_String <- readline("Give a positive query:")
  negQuery_String <- readline("Give a negative query:")
}
```

So this is the basic extraction of data. First we check if a new extraction is needed or not (extract_data == FALSE). In the case that an update of the Abstract database is needed we proceed to the Extract_Data function. This function will read the PubMed .xml file and will create a new database containing information such as Abstracts, Titles, Authors, Keywords, etc. This new Dataframe will be saved for the upcomming preprocessing.
```{r}
# Data extraction

if (extract_data == FALSE & file.exists("Data/Dataframe")){
  df <- readRDS("Data/Dataframe")
}else{
  cat("new extraction \n")
  Extract_Data <- dget("Source/Extract_Data.R")
  df <- Extract_Data(queryPUBMED,abstractSize)
  # export dataframe
  saveRDS(df, file = "Data/Dataframe", ascii = FALSE, version = NULL,
          compress = TRUE, refhook = NULL)
}
```

The Pre-processing here below is where we will perform the tokenisation, removal of stopwords and stemming. It creates the tokensDF dataframe.
```{r}
# Tokenization

if (new_Tokens | file.exists("Data/tokensDF") == FALSE){
  flag0 <- TRUE
  tokenization <- dget("Source/tokenization.R")
  tokensDF <- tokenization(df,stemming,flag0)

  saveRDS(tokensDF, file = "Data/tokensDF", ascii = FALSE, version = NULL,
          compress = TRUE, refhook = NULL)
} else tokensDF <- readRDS("Data/tokensDF")
```

The following is the Latent Semantic Analysis. Firstly we build the TF, IDF and TF-IDF functions. Then we apply them to the term-document matrix (tokendDF, product of the Pre-processing). This gives us the TF-IDF matrix which is a normalized term-document matrix, it gives for each token its importance in the text lowered by its importance in the corpus.
After obtaining the TF-IDF matrix (normalized term-document matrix), the next step is to perform the Single Value Decomposition (SVD) in order to reduce the dimensionality down to 'nv' columns. For this purpose, we use the irlba library. The outputs are three irlba matrix that contain the u: left singular vectors and v: right singular vectors (for more information please read documentation).

```{r}
# LSA 

# if (new_Tokens | new_LSA | file.exists("Data/irlba") == FALSE){
# 
#   ## Data processing (preprocessing & SVD)
#   if (flag) {
#     loadPackage("irlba")
# 
#     # Our function for calculating relative term frequency (TF)
#     term.frequency <- function(row) {
#       row / rowSums(row)
#     }
#     # Our function for calculating inverse document frequency (IDF)
#     inverse.doc.freq <- function(col) {
#       corpus.size <- length(col[,1])
#       doc.count <- colSums(col > 0)
#       log10(corpus.size / doc.count)
#     }
#     # Our function for calculating TF-IDF.
#     tf.idf <- function(tf, idf) {
#       tf * idf
#     }
#     cat("tf-idf \n")
#     
#     
#     # First step, normalize all documents via TF.
#     tokens.tf <- term.frequency(tokensDF)
# 
#     # Second step, calculate the IDF vector that we will use - both
#     tokens.idf <- inverse.doc.freq(tokensDF)
# 
#     # Lastly, calculate TF-IDF for our training corpus.
#     tokens.tfidf <- tf.idf(tokens.tf,tokensDF)
# 
#     #--------------------------------------------------------------#
# 
#     # SVD
#     cat("SVD \n")
#     irlba <- irlba(tokens.tfidf, nv = nv, maxit = 1000)
# 
#     # line names
#     rownames(irlba$v) <- colnames(tokensDF)
#     rownames(irlba$u) <- row.names(tokensDF)
# 
#     saveRDS(irlba, file = "Data/irlba", ascii = FALSE, version = NULL,
#             compress = TRUE, refhook = NULL)
#   } else{
#     LSA <- dget("Source/LSA.R")
#     irlba <- LSA(tokensDF,nv)
#   }
# }else {
#   irlba <- readRDS("Data/irlba")
# }
```

A second method is the Latent Dirichlet Allocation (LDA) a more probabilistic approach.

-- a completer --


```{r}
# LDA 

if (new_LDA | file.exists("Data/LDAtop_terms") == FALSE){
  runLDA <- dget("Source/LDA.R")
  runLDA(k,tokensDF)
}else {
  LDAtop_terms <- readRDS("Data/LDAtop_terms")
  LDAdoc <- readRDS("Data/LDAdoc")
}
```

This last section  allows the experienced user to perform queries in the terminal 

-- a completer --

```{r}
# Query system 
# Abstract <- as.character(df$Abstract)
# 
# if (LSAquery){
#   query_system <- dget("Source/LSA_query_system.R")
# 
#   Result <- query_system(irlba,posQuery_String,negQuery_String,Abstract,stemming)
# 
#   cat("Positive queries:",posQuery_String,"\n","Negative queries:",negQuery_String,"\n")
#   for (i in (1:10)) {
#     cat("Result",i,"\n","Abstract",Result[i],"\n",Abstract[Result[i]],"\n")
#   }
# }
# if (LDAquery){
#   query_system <- dget("Source/LDA_query_system.R")
#   query_system(posQuery_String,negQuery_String,LDAtop_terms,LDAdoc,Abstract)
# }

```

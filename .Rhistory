for (i in 1:Article_Num) {
ID[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["PMID"]])
Abstract[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["Abstract"]])
Title[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["ArticleTitle"]])
Date[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["ArticleDate"]])
Author_lastname[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["AuthorList"]][["Author"]][["LastName"]])
Author_forename[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["AuthorList"]][["Author"]][["ForeName"]])
Author[i] <- paste(Author_lastname[i],Author_forename[i])
}
# create dataframe
df <- data.frame(ID, Abstract, Title, Date, Author)
df <- df[complete.cases(df[ , 2]),] # eliminate NA's in the Abstract column
i=1
while (nchar(as.character(df$Abstract[i]))>100 || nchar(as.character(df$Abstract[i]))<2000) {
print(i)
i<-i+1
}
length(df$ID)
library(XML)
library(easyPubMed)
library(ggplot2)
############### PART 1: Information extraction ###############
## Dataset importation
#---------------------
# Option 1: 698 documents via une requete
#----------
Querry_String <- "AIDS"
Ids <- get_pubmed_ids(Querry_String)
papers <- fetch_pubmed_data(Ids)
# Option 2: 52349 documents via importation du fichier xml.
#----------
# papers <- xmlParse(file = "/home/francois/Documents/Projet_Text_mining/pubmed18n0924.xml")
## Information Extraction from dataset ("papers")
#------------------------------------------------
xmltop = xmlRoot(papers) # top node of "papers" xml structure
Article_Num <- xmlSize(xmltop) # number of nodes (Articles) "in papers"
# xmlSApply(xmltop[[1]], xmlName) # shows names of child nodes
ID <- vector()
Abstract <- vector()
Title <- vector()
Date <- vector()
Author_lastname <- vector()
Author_forename <- vector()
Author <- vector()
for (i in 1:Article_Num) {
ID[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["PMID"]])
Abstract[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["Abstract"]])
Title[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["ArticleTitle"]])
Date[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["ArticleDate"]])
Author_lastname[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["AuthorList"]][["Author"]][["LastName"]])
Author_forename[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["AuthorList"]][["Author"]][["ForeName"]])
Author[i] <- paste(Author_lastname[i],Author_forename[i])
}
# create dataframe
df <- data.frame(ID, Abstract, Title, Date, Author)
df <- df[complete.cases(df[ , 2]),] # eliminate NA's in the Abstract column
length(df$ID)
# Remove too long or too short Abstracts
for (i in 1:length(df$Abstract)) {
if (nchar(as.character(df$Abstract[i]))<100 || nchar(as.character(df$Abstract[i]))>2000) {
print(i)
df <- df[-i,]
}
}
nchar(as.character(df$Abstract[385]))<100 || nchar(as.character(df$Abstract[385]))>2000
nchar(as.character(df$Abstract[384]))<100 || nchar(as.character(df$Abstract[384]))>2000
nchar(as.character(df$Abstract[386]))<100 || nchar(as.character(df$Abstract[386]))>2000
nchar(as.character(df$Abstract[387]))<100 || nchar(as.character(df$Abstract[387]))>2000
nchar(as.character(df$Abstract[387]))
nchar(as.character(df$Abstract[386]))
nchar(as.character(df$Abstract[384]))
nchar(as.character(df$Abstract[383]))
nchar(as.character(df$Abstract[1:100]))
nchar(as.character(df$Abstract[1:400]))
max(nchar(as.character(df$Abstract[1:400])))
min(nchar(as.character(df$Abstract[1:400])))
min(nchar(as.character(df$Abstract[1:90])))
max(nchar(as.character(df$Abstract[1:90])))
max(nchar(as.character(df$Abstract[1:390])))
max(nchar(as.character(df$Abstract[1:395])))
max(nchar(as.character(df$Abstract[1:392])))
max(nchar(as.character(df$Abstract[1:391])))
max(nchar(as.character(df$Abstract[1:390])))
df[as.character(df[ , 2])>200,]
df[as.character(df[ , 2])>2000,]
df[as.character(df[ , 2])>2800,]
df[as.character(df[ , 2])>4000,]
as.character(df[ , 2])
nchar(as.character(df[ , 2]))
df[nchar(as.character(df[ , 2]))>4000,]
df[nchar(as.character(df[ , 2]))>2000,]
df[nchar(as.character(df[ , 3]))>4000,]
df[nchar(as.character(df[ , 2]))>4000,]
df[nchar(as.character(df[ , 2]))>3000,]
df[nchar(as.character(df[ , 2]))<100,]
df[nchar(as.character(df[ , 2]))<200,]
df[nchar(as.character(df[ , 2]))>3000,]
df[nchar(as.character(df[ , 2]))<3000 && nchar(as.character(df[ , 2]))>100,]
# create dataframe
df <- data.frame(ID, Abstract, Title, Date, Author)
# Remove Na's and too long or too short Abstracts.
df <- df[complete.cases(df[ , 2]),]
df <- df[nchar(as.character(df[ , 2]))<3000 && nchar(as.character(df[ , 2]))>100,]
#Just to visualize abstract lengths
a<- vector()
for (i in 1:length(df$Abstract)) {a[i]<-nchar(as.character(df$Abstract[i]))}
plot(a)
plot(a)
nchar(as.character(df$Abstract[,])
)
nchar(as.character(df$Abstract[]))
df$Abstract[382]
nchar(as.character(df$Abstract[382]))
df[nchar(as.character(df[382, 2]))
]
nchar(as.character(df[382, 2]))
nchar(as.character(df[ , 2]))>100
df <- df[nchar(as.character(df[ , 2]))<3000 || nchar(as.character(df[ , 2]))>100,]
#Just to visualize abstract lengths
a<- vector()
for (i in 1:length(df$Abstract)) {a[i]<-nchar(as.character(df$Abstract[i]))}
plot(a)
# create dataframe
df <- data.frame(ID, Abstract, Title, Date, Author)
# Remove Na's and too long or too short Abstracts.
df <- df[complete.cases(df[ , 2]),]
df <- df[nchar(as.character(df[ , 2]))<3000 || nchar(as.character(df[ , 2]))>100,]
#Just to visualize abstract lengths
a<- vector()
for (i in 1:length(df$Abstract)) {a[i]<-nchar(as.character(df$Abstract[i]))}
plot(a)
# create dataframe
df <- data.frame(ID, Abstract, Title, Date, Author)
# Remove Na's and too long or too short Abstracts.
df <- df[complete.cases(df[ , 2]),]
df <- df[nchar(as.character(df[ , 2]))<3000 || nchar(as.character(df[ , 2]))>100,]
#Just to visualize abstract lengths
a<- vector()
for (i in 1:length(df$Abstract)) {a[i]<-nchar(as.character(df$Abstract[i]))}
plot(a)
nchar(as.character(df[382, 2]))
df <- df[nchar(as.character(df[ , 2]))<3000 & nchar(as.character(df[ , 2]))>100,]
#Just to visualize abstract lengths
a<- vector()
for (i in 1:length(df$Abstract)) {a[i]<-nchar(as.character(df$Abstract[i]))}
plot(a)
nchar(as.character(df[382, 2]))
TRUE & FALSE
TRUE && FALSE
TRUE || FALSE
TRUE | FALSE
TRUE && FALSE
TRUE & FALSE
df <- df[nchar(as.character(df[ , 2]))<3000 && nchar(as.character(df[ , 2]))>100,]
#Just to visualize abstract lengths
a<- vector()
for (i in 1:length(df$Abstract)) {a[i]<-nchar(as.character(df$Abstract[i]))}
plot(a)
Abstract <- as.character(df$Abstract)
library(XML)
library(easyPubMed)
library(ggplot2)
############### PART 1: Information extraction ###############
## Dataset importation
#---------------------
# Option 1: 698 documents via une requete
#----------
Querry_String <- "AIDS"
Ids <- get_pubmed_ids(Querry_String)
papers <- fetch_pubmed_data(Ids)
# Option 2: 52349 documents via importation du fichier xml.
#----------
# papers <- xmlParse(file = "/home/francois/Documents/Projet_Text_mining/pubmed18n0924.xml")
## Information Extraction from dataset ("papers")
#------------------------------------------------
xmltop = xmlRoot(papers) # top node of "papers" xml structure
Article_Num <- xmlSize(xmltop) # number of nodes (Articles) "in papers"
# xmlSApply(xmltop[[1]], xmlName) # shows names of child nodes
ID <- vector()
Abstract <- vector()
Title <- vector()
Date <- vector()
Author_lastname <- vector()
Author_forename <- vector()
Author <- vector()
for (i in 1:Article_Num) {
ID[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["PMID"]])
Abstract[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["Abstract"]])
Title[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["ArticleTitle"]])
Date[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["ArticleDate"]])
Author_lastname[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["AuthorList"]][["Author"]][["LastName"]])
Author_forename[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["AuthorList"]][["Author"]][["ForeName"]])
Author[i] <- paste(Author_lastname[i],Author_forename[i])
}
# create dataframe
df <- data.frame(ID, Abstract, Title, Date, Author)
# Remove Na's and too long or too short Abstracts.
df <- df[complete.cases(df[ , 2]),]
df <- df[nchar(as.character(df[ , 2]))<3000 && nchar(as.character(df[ , 2]))>100,]
#Just to visualize abstract lengths
a<- vector()
for (i in 1:length(df$Abstract)) {a[i]<-nchar(as.character(df$Abstract[i]))}
plot(a)
nchar(as.character(df[ , 2]))<3000 && nchar(as.character(df[ , 2]))>100
nchar(as.character(df[ , 2]))<3000 & nchar(as.character(df[ , 2]))>100
# create dataframe
df <- data.frame(ID, Abstract, Title, Date, Author)
# Remove Na's and too long or too short Abstracts.
df <- df[complete.cases(df[ , 2]),]
df <- df[nchar(as.character(df[ , 2]))<3000 & nchar(as.character(df[ , 2]))>100,]
#Just to visualize abstract lengths
a<- vector()
for (i in 1:length(df$Abstract)) {a[i]<-nchar(as.character(df$Abstract[i]))}
plot(a)
min(nchar(as.character(df$Abstract[i])))
min(nchar(as.character(df$Abstract)))
wich.min(nchar(as.character(df$Abstract)))
which.min(nchar(as.character(df$Abstract)))
df$Abstract[216]
nchar("Peter Godfrey-Faussett and colleagues present six epidemiological metrics for tracking progress in reducing the public health threat of HIV."
)
Abstract <- as.character(df$Abstract)
NbrDoc<-100
# Tokenize
tokens <- tokens(Abstract, what = "word",
remove_numbers = TRUE, remove_punct = TRUE,
remove_symbols = TRUE, remove_hyphens = FALSE)
# Tokenize
tokens <- tokens(Abstract, what = "word",
remove_numbers = TRUE, remove_punct = TRUE,
remove_symbols = TRUE, remove_hyphens = FALSE)
## Approche Bag of words:
library(quanteda)
Abstract <- as.character(df$Abstract)
NbrDoc<-100
# Tokenize
tokens <- tokens(Abstract, what = "word",
remove_numbers = TRUE, remove_punct = TRUE,
remove_symbols = TRUE, remove_hyphens = FALSE)
# Tokenize
tokens <- tokens(Abstract, what = "word",
remove_numbers = TRUE, remove_punct = TRUE,
remove_symbols = TRUE, remove_hyphens = FALSE)
# minimize capital letters
tokens <- tokens_tolower(tokens)
# stopwords
stop<-stopwords()
new_stopwords<-append(stop,c("fig.","eq.","e.g"))
tokens <- tokens_select(tokens, new_stopwords, selection = "remove")
tokens <- tokens_select(tokens,min_nchar = 2, selection ="keep")
library(XML)
library(easyPubMed)
library(ggplot2)
############### PART 1: Information extraction ###############
## Dataset importation
#---------------------
# Option 1: 698 documents via une requete
#----------
Querry_String <- "AIDS"
Ids <- get_pubmed_ids(Querry_String)
papers <- fetch_pubmed_data(Ids)
# Option 2: 52349 documents via importation du fichier xml.
#----------
# papers <- xmlParse(file = "/home/francois/Documents/Projet_Text_mining/pubmed18n0924.xml")
## Information Extraction from dataset ("papers")
#------------------------------------------------
xmltop = xmlRoot(papers) # top node of "papers" xml structure
Article_Num <- xmlSize(xmltop) # number of nodes (Articles) "in papers"
# xmlSApply(xmltop[[1]], xmlName) # shows names of child nodes
ID <- vector()
Abstract <- vector()
Title <- vector()
Date <- vector()
Author_lastname <- vector()
Author_forename <- vector()
Author <- vector()
for (i in 1:Article_Num) {
ID[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["PMID"]])
Abstract[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["Abstract"]])
Title[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["ArticleTitle"]])
Date[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["ArticleDate"]])
Author_lastname[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["AuthorList"]][["Author"]][["LastName"]])
Author_forename[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["AuthorList"]][["Author"]][["ForeName"]])
Author[i] <- paste(Author_lastname[i],Author_forename[i])
}
# create dataframe
df <- data.frame(ID, Abstract, Title, Date, Author)
# Remove Na's and too long or too short Abstracts.
df <- df[complete.cases(df[ , 2]),]
df <- df[nchar(as.character(df[ , 2]))<3000 & nchar(as.character(df[ , 2]))>100,]
# visualize abstract lengths
a<- vector()
for (i in 1:length(df$Abstract)) {a[i]<-nchar(as.character(df$Abstract[i]))}
plot(a)
############### PART 2: text mining  ###############
## Approche Bag of words:
library(quanteda)
Abstract <- as.character(df$Abstract)
NbrDoc<-100
# Tokenize
tokens <- tokens(Abstract, what = "word",
remove_numbers = TRUE, remove_punct = TRUE,
remove_symbols = TRUE, remove_hyphens = FALSE)
# for bigrams.
# test.tokens <- tokens_ngrams(test.tokens, n = 1:2)
tokens
length(tokens)
tok1<-tokens
# minimize capital letters
tokens <- tokens_tolower(tokens)
tokens
tok2<-tokens
library(XML)
library(easyPubMed)
library(ggplot2)
############### PART 1: Information extraction ###############
## Dataset importation
#---------------------
# Option 1: 698 documents via une requete
#----------
Querry_String <- "AIDS"
Ids <- get_pubmed_ids(Querry_String)
papers <- fetch_pubmed_data(Ids)
# Option 2: 52349 documents via importation du fichier xml.
#----------
# papers <- xmlParse(file = "/home/francois/Documents/Projet_Text_mining/pubmed18n0924.xml")
## Information Extraction from dataset ("papers")
#------------------------------------------------
xmltop = xmlRoot(papers) # top node of "papers" xml structure
Article_Num <- xmlSize(xmltop) # number of nodes (Articles) "in papers"
# xmlSApply(xmltop[[1]], xmlName) # shows names of child nodes
ID <- vector()
Abstract <- vector()
Title <- vector()
Date <- vector()
Author_lastname <- vector()
Author_forename <- vector()
Author <- vector()
for (i in 1:Article_Num) {
ID[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["PMID"]])
Abstract[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["Abstract"]])
Title[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["ArticleTitle"]])
Date[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["ArticleDate"]])
Author_lastname[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["AuthorList"]][["Author"]][["LastName"]])
Author_forename[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["AuthorList"]][["Author"]][["ForeName"]])
Author[i] <- paste(Author_lastname[i],Author_forename[i])
}
# create dataframe
df <- data.frame(ID, Abstract, Title, Date, Author)
# Remove Na's and too long or too short Abstracts.
df <- df[complete.cases(df[ , 2]),]
df <- df[nchar(as.character(df[ , 2]))<3000 & nchar(as.character(df[ , 2]))>100,]
# visualize abstract lengths
a<- vector()
for (i in 1:length(df$Abstract)) {a[i]<-nchar(as.character(df$Abstract[i]))}
plot(a)
############### PART 2: text mining  ###############
## Approche Bag of words:
library(quanteda)
Abstract <- as.character(df$Abstract)
NbrDoc<-100
# Tokenize
tokens <- tokens(Abstract, what = "word",
remove_numbers = TRUE, remove_punct = TRUE,
remove_symbols = TRUE, remove_hyphens = FALSE)
# for bigrams.
# test.tokens <- tokens_ngrams(test.tokens, n = 1:2)
# stopwords
stop<-stopwords()
# Create our first bag-of-words model.
tokens.dfm <- dfm(tokens, tolower = FALSE)
# Create our first bag-of-words model.
tokens.dfm
View(tokens.dfm)
View(t(tokens.dfm))
# Create our first bag-of-words model.
tokens.dfm <- dfm(tokens, tolower = TRUE)
View(t(tokens.dfm))
# Tokenize
tokens <- tokens(Abstract, what = "word",
remove_numbers = TRUE, remove_punct = TRUE,
remove_symbols = TRUE, remove_hyphens = FALSE,
tolower = TRUE)
?tokens
tokens
library(XML)
library(easyPubMed)
library(ggplot2)
############### PART 1: Information extraction ###############
## Dataset importation
#---------------------
# Option 1: 698 documents via une requete
#----------
Querry_String <- "AIDS"
Ids <- get_pubmed_ids(Querry_String)
papers <- fetch_pubmed_data(Ids)
# Option 2: 52349 documents via importation du fichier xml.
#----------
# papers <- xmlParse(file = "/home/francois/Documents/Projet_Text_mining/pubmed18n0924.xml")
## Information Extraction from dataset ("papers")
#------------------------------------------------
xmltop = xmlRoot(papers) # top node of "papers" xml structure
Article_Num <- xmlSize(xmltop) # number of nodes (Articles) "in papers"
# xmlSApply(xmltop[[1]], xmlName) # shows names of child nodes
ID <- vector()
Abstract <- vector()
Title <- vector()
Date <- vector()
Author_lastname <- vector()
Author_forename <- vector()
Author <- vector()
for (i in 1:Article_Num) {
ID[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["PMID"]])
Abstract[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["Abstract"]])
Title[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["ArticleTitle"]])
Date[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["ArticleDate"]])
Author_lastname[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["AuthorList"]][["Author"]][["LastName"]])
Author_forename[i] <- xmlValue(xmltop[[i]][["MedlineCitation"]][["Article"]][["AuthorList"]][["Author"]][["ForeName"]])
Author[i] <- paste(Author_lastname[i],Author_forename[i])
}
# create dataframe
df <- data.frame(ID, Abstract, Title, Date, Author)
# Remove Na's and too long or too short Abstracts.
df <- df[complete.cases(df[ , 2]),]
df <- df[nchar(as.character(df[ , 2]))<3000 & nchar(as.character(df[ , 2]))>100,]
# visualize abstract lengths
a<- vector()
for (i in 1:length(df$Abstract)) {a[i]<-nchar(as.character(df$Abstract[i]))}
plot(a)
############### PART 2: text mining  ###############
## Approche Bag of words:
library(quanteda)
Abstract <- as.character(df$Abstract)
NbrDoc<-100
# Tokenize
tokens <- tokens(Abstract, what = "word",
remove_numbers = TRUE, remove_punct = TRUE,
remove_symbols = TRUE, remove_hyphens = FALSE)
# for bigrams.
# test.tokens <- tokens_ngrams(test.tokens, n = 1:2)
# minimize capital letters
tokens <- tokens_tolower(tokens)
# stopwords
stop<-stopwords()
new_stopwords<-append(stop,c("fig.","eq.","e.g"))
tokens <- tokens_select(tokens, new_stopwords, selection = "remove")
tokens <- tokens_select(tokens,min_nchar = 2, selection ="keep")
# stem
# tokens <- tokens_wordstem(tokens, language = "english")
# print(tokens)
# Create our first bag-of-words model.
tokens.dfm <- dfm(tokens, tolower = TRUE)
# Transform to a matrix and inspect.
tokens.matrix <- as.matrix(tokens.dfm)
# View(tokens.matrix[1:NbrDoc, 1:100])
# dim(tokens.matrix)
# Tokenfrequence
# In corpus
freq <- sort(colSums(tokens.matrix), decreasing=TRUE)
wf <- data.frame(word=names(freq), freq=freq)
# In specific document
Doc<-5
freqInDoc <- sort(tokens.matrix[Doc,], decreasing=TRUE)
wfindoc <- data.frame(word=names(freqInDoc), freq=freqInDoc)
# plot word frequence
# pl <- ggplot(subset(wf, freq > 1) ,aes(word, freq))
# # pl <- ggplot(subset(wfindoc, freq > 1) ,aes(word, freq))
# pl <- pl + geom_bar(stat="identity", fill="darkred", colour="white")
# pl + theme(axis.text.x=element_text(angle=90, hjust=1)) + ggtitle("Uni-Gram Frequency")
# Word Cloud
# library(wordcloud)
# set.seed(100)
# wordcloud(names(freq), freq, min.freq=2, colors=brewer.pal(6, "Dark2"))
# Our function for calculating relative term frequency (TF)
term.frequency <- function(row) {
row / sum(row)
}
# Our function for calculating inverse document frequency (IDF)
inverse.doc.freq <- function(col) {
corpus.size <- length(col)
doc.count <- length(which(col > 0))
log10(corpus.size / doc.count)
}
# Our function for calculating TF-IDF.
tf.idf <- function(x, idf) {
x * idf
}
# First step, normalize all documents via TF.
tokens.df <- apply(tokens.matrix, 1, term.frequency)
# dim(tokens.df)
# View(tokens.df[1:100, 1:NbrDoc])
# Second step, calculate the IDF vector that we will use - both
tokens.idf <- apply(tokens.matrix, 2, inverse.doc.freq)
str(tokens.idf)
# Lastly, calculate TF-IDF for our training corpus.
tokens.tfidf <-  apply(tokens.df, 2, tf.idf, idf = tokens.idf)
# dim(tokens.tfidf)
# View(tokens.tfidf[1:25, 1:NbrDoc])
# Transpose the matrix
tokens.tfidf <- t(tokens.tfidf)
# dim(tokens.tfidf)
# View(tokens.tfidf[1:NbrDoc, 1:25])
# Check for incopmlete cases.
incomplete.cases <- which(!complete.cases(tokens.tfidf))
# Abstract[incomplete.cases]
# Fix incomplete cases
tokens.tfidf[incomplete.cases,] <- rep(0.0, ncol(tokens.tfidf))
# dim(tokens.tfidf)
# sum(which(!complete.cases(tokens.tfidf)))
# Make a clean data frame.
tokens.tfidf.df <- data.frame(tokens.tfidf)
names(tokens.tfidf.df) <- make.names(names(tokens.tfidf.df))
